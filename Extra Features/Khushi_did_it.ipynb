{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3115 images belonging to 36 classes.\n",
      "Epoch 1/10\n",
      "15/98 [===>..........................] - ETA: 1:17 - loss: 3.2389 - accuracy: 0.2083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khush\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 99s 989ms/step - loss: 1.7577 - accuracy: 0.5165\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 101s 1s/step - loss: 0.9356 - accuracy: 0.7014\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 98s 1s/step - loss: 0.7539 - accuracy: 0.7583\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 98s 995ms/step - loss: 0.6408 - accuracy: 0.8010\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 97s 989ms/step - loss: 0.6162 - accuracy: 0.7897\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 98s 999ms/step - loss: 0.5493 - accuracy: 0.8222\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 98s 1s/step - loss: 0.5143 - accuracy: 0.8398\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 98s 998ms/step - loss: 0.4622 - accuracy: 0.8433\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 98s 1s/step - loss: 0.4460 - accuracy: 0.8379\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 98s 996ms/step - loss: 0.3991 - accuracy: 0.8626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khush\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = 36  # Number of fruits and vegetables\n",
    "\n",
    "# Create an instance of the MobileNetV2 model with pretrained weights\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers of the pretrained model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add a custom classifier on top of the base model\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "predictions = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Data augmentation for the training dataset\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'train/',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=None  # You can provide a validation dataset here if available\n",
    ")\n",
    "\n",
    "# Save the model for future use\n",
    "model.save('fruit_vegetable_classifier.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 833ms/step\n",
      "The predicted class is: pomegranate\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('fruit_vegetable_classifier.h5')\n",
    "\n",
    "# Load and preprocess the input imagPomegranate-_115640806.jpg'\n",
    "img_path = 'Pomegranate-_115640806.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "img = image.img_to_array(img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "img = img / 255.0  # Rescale the image\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(img)\n",
    "class_index = np.argmax(predictions)\n",
    "class_name = class_names[class_index]  # You should have a list of class names\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
