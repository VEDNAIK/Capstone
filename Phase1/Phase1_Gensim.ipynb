{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 60] Operation\n",
      "[nltk_data]     timed out>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import string \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import unidecode\n",
    "import nltk.corpus\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/ved/Desktop/Capstone/Dataset/Cleaned_Indian_Food_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1m/v10vjxqd235fn30s91s6nnx40000gn/T/ipykernel_38338/1364071601.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"ingredients\"][i]=re.sub(\"\\(.*?\\)\",\"\",df[\"ingredients\"][i])\n",
      "/var/folders/1m/v10vjxqd235fn30s91s6nnx40000gn/T/ipykernel_38338/1364071601.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"ingredients\"][i] = df[\"ingredients\"][i].replace('(','').replace(')','')\n",
      "/var/folders/1m/v10vjxqd235fn30s91s6nnx40000gn/T/ipykernel_38338/1364071601.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"ingredients\"][i] = df[\"ingredients\"][i].split(',')\n"
     ]
    }
   ],
   "source": [
    "df.rename(columns = {'Cleaned-Ingredients':'ingredients'}, inplace = True)\n",
    "for i in range(len(df[\"ingredients\"])):\n",
    "    df[\"ingredients\"][i]=re.sub(\"\\(.*?\\)\",\"\",df[\"ingredients\"][i])\n",
    "    df[\"ingredients\"][i] = df[\"ingredients\"][i].replace('(','').replace(')','')\n",
    "    df[\"ingredients\"][i] = df[\"ingredients\"][i].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingredient_parser(ingredients):\n",
    "    # measures and common words (already lemmatized)   \n",
    "    measures = ['teaspoon', 't', 'tsp.', 'tablespoon', 'T', ...]\n",
    "    words_to_remove = ['fresh', 'a', 'red', 'bunch', ...]\n",
    "    # Turn ingredient list from string into a list \n",
    "    if isinstance(ingredients, list):\n",
    "       pass\n",
    "    else:\n",
    "       ingredients = ast.literal_eval(ingredients)\n",
    "    # We first get rid of all the punctuation\n",
    "    remove_punctuations = str.maketrans('', '', string.punctuation)\n",
    "    # initialize nltk's lemmatizer    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ingred_list = []\n",
    "    for each_item in ingredients:\n",
    "        each_item.translate(remove_punctuations)\n",
    "        # We split up with hyphens as well as spaces\n",
    "        items = re.split(' |-', each_item)\n",
    "        # Get rid of words containing non alphabet letters\n",
    "        items = [word for word in items if word.isalpha()]\n",
    "        # Turn everything to lowercase\n",
    "        items = [word.lower() for word in items]\n",
    "        # remove accents\n",
    "        items = [unidecode.unidecode(word) for word in items]\n",
    "        # Lemmatize words so we can compare words to measuring words\n",
    "        items = [lemmatizer.lemmatize(word) for word in items]\n",
    "        # get rid of stop words\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        items = [word for word in items if word not in stop_words]\n",
    "        # Gets rid of measuring words/phrases, e.g. heaped teaspoon\n",
    "        items = [word for word in items if word not in measures]\n",
    "        # Get rid of common easy words\n",
    "        items = [word for word in items if word not in words_to_remove]\n",
    "        if items:\n",
    "           ingred_list.append(' '.join(items))\n",
    "    return ingred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get corpus with the documents sorted in alphabetical order\n",
    "def get_and_sort_corpus(data):\n",
    "    corpus_sorted = []\n",
    "    for doc in data.parsed.values:\n",
    "        doc.sort()\n",
    "        corpus_sorted.append(doc)\n",
    "    return corpus_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['parsed'] = df.ingredients.apply(ingredient_parser)\n",
    "# get corpus\n",
    "#corpus = get_and_sort_corpus(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"chicken, egg, tomato, onion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "def get_recommendations(input):\n",
    "\n",
    "# Example data\n",
    "    recipe_names = df['TranslatedRecipeName']\n",
    "    ingredients = df['parsed']\n",
    "\n",
    "# Tokenize and preprocess ingredients\n",
    "    ingredient_tokens = ingredients\n",
    "\n",
    "# Train FastText embeddings\n",
    "    model = gensim.models.FastText(ingredient_tokens, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Create embeddings for a query ingredient list\n",
    "    query_ingredients = \"oil, onion, tomato, ginger, garlic, red chilli powder, turmeric, salt, corriander powder,garam masala, paneer\"\n",
    "    query_embedding = np.mean([model.wv[token] for token in query_ingredients.split(', ')], axis=0)\n",
    "\n",
    "# Compute cosine similarities\n",
    "    similarities = [cosine_similarity([query_embedding], [np.mean([model.wv[token] for token in ing], axis=0)])[0][0] for ing in ingredient_tokens]\n",
    "\n",
    "# Recommend recipes based on similarities\n",
    "    recommendations = [recipe_names[i] for i in np.argsort(similarities)[::-1]]\n",
    "    score = sorted(similarities, reverse=True)\n",
    "    #print(\"Recommended Recipes:\")\n",
    "    #for i in range(len(recommendations)):\n",
    "     #   print(recommendations[i],score[i])\n",
    "# Calculate Precision at K (P@K) for K=3\n",
    "    test = pd.read_csv(\"/Users/ved/Desktop/Capstone/accuracytest_-_Sheet1_1.csv\")\n",
    "    actual_recipes = list(test[\"TranslatedRecipeName\"])\n",
    "    K = 10  \n",
    "    # Actual recipes relevant to the query\n",
    "\n",
    "# Calculate P@K\n",
    "    correct_recommendations = [recipe in recommendations[:K] for recipe in actual_recipes]\n",
    "    precision_at_K = sum(correct_recommendations) / K\n",
    "    print(f\"Precision at {K} using GENSIM: {precision_at_K}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision at 10 using GENSIM: 0.1\n"
     ]
    }
   ],
   "source": [
    "x = get_recommendations(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
