{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 60] Operation\n",
      "[nltk_data]     timed out>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import string \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import unidecode\n",
    "import nltk.corpus\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import config\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/ved/Desktop/Capstone/Dataset/Cleaned_Indian_Food_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1m/v10vjxqd235fn30s91s6nnx40000gn/T/ipykernel_38337/1364071601.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"ingredients\"][i]=re.sub(\"\\(.*?\\)\",\"\",df[\"ingredients\"][i])\n",
      "/var/folders/1m/v10vjxqd235fn30s91s6nnx40000gn/T/ipykernel_38337/1364071601.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"ingredients\"][i] = df[\"ingredients\"][i].replace('(','').replace(')','')\n",
      "/var/folders/1m/v10vjxqd235fn30s91s6nnx40000gn/T/ipykernel_38337/1364071601.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"ingredients\"][i] = df[\"ingredients\"][i].split(',')\n"
     ]
    }
   ],
   "source": [
    "df.rename(columns = {'Cleaned-Ingredients':'ingredients'}, inplace = True)\n",
    "for i in range(len(df[\"ingredients\"])):\n",
    "    df[\"ingredients\"][i]=re.sub(\"\\(.*?\\)\",\"\",df[\"ingredients\"][i])\n",
    "    df[\"ingredients\"][i] = df[\"ingredients\"][i].replace('(','').replace(')','')\n",
    "    df[\"ingredients\"][i] = df[\"ingredients\"][i].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Users/ved/Desktop/cap/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['salt', 'amchur ', 'karela ', 'red chilli powder', 'gram flour ', 'onion', 'cumin seeds ', 'coriander powder', 'turmeric powder', 'sunflower oil']\n"
     ]
    }
   ],
   "source": [
    "print(df[\"ingredients\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingredient_parser(ingredients):\n",
    "    # measures and common words (already lemmatized)   \n",
    "    measures = ['teaspoon', 't', 'tsp.', 'tablespoon', 'T', ...]\n",
    "    words_to_remove = ['fresh', 'a', 'red', 'bunch', ...]\n",
    "    # Turn ingredient list from string into a list \n",
    "    if isinstance(ingredients, list):\n",
    "       pass\n",
    "    else:\n",
    "       ingredients = ast.literal_eval(ingredients)\n",
    "    # We first get rid of all the punctuation\n",
    "    remove_punctuations = str.maketrans('', '', string.punctuation)\n",
    "    # initialize nltk's lemmatizer    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ingred_list = []\n",
    "    for each_item in ingredients:\n",
    "        each_item.translate(remove_punctuations)\n",
    "        # We split up with hyphens as well as spaces\n",
    "        items = re.split(' |-', each_item)\n",
    "        # Get rid of words containing non alphabet letters\n",
    "        items = [word for word in items if word.isalpha()]\n",
    "        # Turn everything to lowercase\n",
    "        items = [word.lower() for word in items]\n",
    "        # remove accents\n",
    "        items = [unidecode.unidecode(word) for word in items]\n",
    "        # Lemmatize words so we can compare words to measuring words\n",
    "        items = [lemmatizer.lemmatize(word) for word in items]\n",
    "        # get rid of stop words\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        items = [word for word in items if word not in stop_words]\n",
    "        # Gets rid of measuring words/phrases, e.g. heaped teaspoon\n",
    "        items = [word for word in items if word not in measures]\n",
    "        # Get rid of common easy words\n",
    "        items = [word for word in items if word not in words_to_remove]\n",
    "        if items:\n",
    "           ingred_list.append(' '.join(items))\n",
    "    return ingred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['parsed'] = df.ingredients.apply(ingredient_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingredient_parser_final(ingredient):\n",
    "    \"\"\"\n",
    "    neaten the ingredients being outputted\n",
    "    \"\"\"\n",
    "    if isinstance(ingredient, list):\n",
    "        ingredients = ingredient\n",
    "    else:\n",
    "        ingredients = ast.literal_eval(ingredient)\n",
    "\n",
    "    ingredients = \",\".join(ingredients)\n",
    "    ingredients = unidecode.unidecode(ingredients)\n",
    "    return ingredients\n",
    "def title_parser(title):\n",
    "    title = unidecode.unidecode(title)\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 191 calls to <function Model.make_predict_function.<locals>.predict_function at 0x2bb4e8c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "186/186 [==============================] - 0s 266us/step\n",
      "186/186 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_ingredients(df, column_name):\n",
    "    # Extract the ingredient column from the DataFrame\n",
    "    ingredient_lists = df[column_name].tolist()\n",
    "    \n",
    "    # Tokenize and create word embeddings\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(ingredient_lists)\n",
    "    vocab_size = len(tokenizer.word_index) + 1  # +1 for OOV token\n",
    "\n",
    "    # Convert ingredients to sequences\n",
    "    sequences = tokenizer.texts_to_sequences(ingredient_lists)\n",
    "\n",
    "    # Pad sequences for consistent input size\n",
    "    max_sequence_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    # Create a model for word embeddings\n",
    "    word_embedding_model = Sequential()\n",
    "    word_embedding_model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_sequence_length))\n",
    "    word_embedding_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    word_embeddings = word_embedding_model.predict(padded_sequences)\n",
    "\n",
    "    # Create a model for document embeddings\n",
    "    document_embedding_model = Sequential()\n",
    "    document_embedding_model.add(LSTM(100))\n",
    "    document_embedding_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    document_embeddings = document_embedding_model.predict(word_embeddings)\n",
    "\n",
    "    # Add the embeddings to the DataFrame\n",
    "    df['word_embeddings'] = list(word_embeddings)\n",
    "    df['document_embeddings'] = list(document_embeddings)\n",
    "\n",
    "# Process the ingredients column in the DataFrame\n",
    "process_ingredients(df, 'parsed')\n",
    "\n",
    "# Display the updated DataFrame\n",
    "\n",
    "\n",
    "def process_ingredients_input(abc):\n",
    "    # Extract the ingredient column from the DataFrame\n",
    "    \n",
    "    # Tokenize and create word embeddings\n",
    "    ingredient_lists = abc\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(ingredient_lists)\n",
    "    vocab_size = len(tokenizer.word_index) + 1  # +1 for OOV token\n",
    "\n",
    "    # Convert ingredients to sequences\n",
    "    sequences = tokenizer.texts_to_sequences(ingredient_lists)\n",
    "\n",
    "    # Pad sequences for consistent input size\n",
    "    max_sequence_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    # Create a model for word embeddings\n",
    "    word_embedding_model = Sequential()\n",
    "    word_embedding_model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_sequence_length))\n",
    "    word_embedding_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    word_embeddings = word_embedding_model.predict(padded_sequences)\n",
    "\n",
    "    # Create a model for document embeddings\n",
    "    document_embedding_model = Sequential()\n",
    "    document_embedding_model.add(LSTM(100))\n",
    "    document_embedding_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    document_embeddings = document_embedding_model.predict(word_embeddings)\n",
    "\n",
    "    # Add the embeddings to the DataFrame\n",
    "    a['word_embeddings'] = list(word_embeddings)\n",
    "    a['document_embeddings'] = list(document_embeddings)\n",
    "\n",
    "# Process the ingredients column in the DataFrame\n",
    "# Display the updated DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(N, scores):\n",
    "    \"\"\"\n",
    "    Rank scores and output a pandas data frame containing all the details of the top N recipes.\n",
    "    :param scores: list of cosine similarities\n",
    "    \"\"\"\n",
    "    # load in recipe dataset\n",
    "    df_recipes = df.copy()\n",
    "    # order the scores with and filter to get the highest N scores\n",
    "    top = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:N]\n",
    "    # create dataframe to load in recommendations\n",
    "    recommendation = pd.DataFrame(columns=[\"TranslatedRecipeName\", \"ingredients\", \"URL\",\"score\",\"cuisine\"])\n",
    "    count = 0\n",
    "    for i in top:\n",
    "        recommendation.at[count, \"TranslatedRecipeName\"] = title_parser(df_recipes[\"TranslatedRecipeName\"][i])\n",
    "        recommendation.at[count, \"ingredients\"] = ingredient_parser_final(\n",
    "            df_recipes[\"ingredients\"][i]\n",
    "        )\n",
    "        recommendation.at[count, \"URL\"] = df_recipes[\"URL\"][i]\n",
    "        recommendation.at[count, \"cuisine\"] = df_recipes[\"Cuisine\"][i]\n",
    "        recommendation.at[count, \"score\"] = f\"{scores[i]}\"\n",
    "        count += 1\n",
    "    return recommendation\n",
    "\n",
    "def get_recs(ingredients, N=5, mean=False):\n",
    "    \"\"\"\n",
    "    Get the top N recipe recomendations.\n",
    "    :param ingredients: comma seperated string listing ingredients\n",
    "    :param N: number of recommendations\n",
    "    :param mean: False if using tfidf weighted embeddings, True if using simple mean\n",
    "    \n",
    "\n",
    "    data=df.copy()\n",
    "    # parse ingredients\n",
    "    data[\"parsed\"] = data.ingredients.apply(ingredient_parser)\n",
    "    # create corpus\n",
    "    corpus = get_and_sort_corpus(data)\n",
    "\n",
    "    if mean:\n",
    "        # get average embdeddings for each document\n",
    "        mean_vec_tr = MeanEmbeddingVectorizer(model)\n",
    "        doc_vec = mean_vec_tr.transform(corpus)\n",
    "        doc_vec = [doc.reshape(1, -1) for doc in doc_vec]\n",
    "        assert len(doc_vec) == len(corpus)\n",
    "    else:\n",
    "        # use TF-IDF as weights for each word embedding\n",
    "        tfidf_vec_tr = TfidfEmbeddingVectorizer(model)\n",
    "        tfidf_vec_tr.fit(corpus)\n",
    "        doc_vec = tfidf_vec_tr.transform(corpus)\n",
    "        doc_vec = [doc.reshape(1, -1) for doc in doc_vec]\n",
    "        assert len(doc_vec) == len(corpus)\n",
    "    \"\"\"\n",
    "\n",
    "    # create embeddings for input text\n",
    "    input = ingredients\n",
    "    # create tokens with elements\n",
    "    input = input.split(\",\")\n",
    "    # parse ingredient list\n",
    "    input = ingredient_parser(input)\n",
    "    input = process_ingredients_input(input)\n",
    "    # get embeddings for ingredient doc\n",
    "    \n",
    "\n",
    "    # get cosine similarity between input embedding and all the document embeddings\n",
    "    a_document_embeddings = a['document_embeddings'].values[0]\n",
    "\n",
    "# Calculate cosine similarities using a list comprehension\n",
    "    cos_sim = [cosine_similarity([a_document_embeddings], x.reshape(1, -1))[0][0] for x in df['document_embeddings']]\n",
    "\n",
    "\n",
    "# Assuming 'a' contains the document embeddings you want to compare to 'df'\n",
    "    #a_document_embeddings = a['document_embeddings'].values[0]\n",
    "\n",
    "# Calculate cosine similarities using a list comprehension\n",
    "# 'cos_sim' will now contain the cosine similarities\n",
    "    scores = list(cos_sim)\n",
    "    # Filter top N recommendations\n",
    "    recommendations = get_recommendations(N, scores)\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 240ms/step\n",
      "                                TranslatedRecipeName  \\\n",
      "0                  Iyengar Style Thavala Adai Recipe   \n",
      "1  Thandai Phirni Recipe - A Delicious Indian Ric...   \n",
      "2                         Herbed Dinner Rolls Recipe   \n",
      "3                           Bengali Steam Doi Recipe   \n",
      "4                         Mushroom Fried Rice Recipe   \n",
      "\n",
      "                                         ingredients  \\\n",
      "0  salt,arhar dal ,rice ,mustard seeds,methi powd...   \n",
      "1  thandai powder,cashew nuts,rice,pistachios,saf...   \n",
      "2  salt,eggs fork,milk,water,dry yeast,herbs,viva...   \n",
      "3  hung curd ,pistachios,milk,raisins,strawberrie...   \n",
      "4  basmati rice,spring onion greens,ginger,salt,g...   \n",
      "\n",
      "                                                 URL                score  \\\n",
      "0  https://www.archanaskitchen.com/iyengar-style-...  0.24577851593494415   \n",
      "1  https://www.archanaskitchen.com/thandai-phirni...  0.24188458919525146   \n",
      "2  https://www.archanaskitchen.com/herbed-dinner-...   0.2388283908367157   \n",
      "3  https://www.archanaskitchen.com/bengali-bhapa-...  0.23873993754386902   \n",
      "4  https://www.archanaskitchen.com/mushroom-fried...  0.23682457208633423   \n",
      "\n",
      "           cuisine  \n",
      "0       Tamil Nadu  \n",
      "1           Indian  \n",
      "2         European  \n",
      "3  Bengali Recipes  \n",
      "4            Asian  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # test \n",
    "    input = \"chicken, egg, tomato, onion\"\n",
    "    rec = get_recs(input)\n",
    "    print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"/Users/ved/Desktop/Capstone/accuracytest_-_Sheet1_1.csv\")\n",
    "actual_recipes = list(test[\"TranslatedRecipeName\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_at_K ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision at 10 using LSTM: 0.0\n"
     ]
    }
   ],
   "source": [
    "K = 10 # mention it as actual no. of recipies\n",
    "# Calculate P@K\n",
    "correct_recommendations=[]\n",
    "for recipe in actual_recipes:\n",
    "    for i in rec['TranslatedRecipeName'][:K]:\n",
    "        if recipe == i:\n",
    "            correct_recommendations.append(True)\n",
    "#correct_recommendations = [recipe in rec['TranslatedRecipeName'][:K] for recipe in actual_recipes]\n",
    "precision_at_K = sum(correct_recommendations) / K\n",
    "\n",
    "print(f\"Precision at {K} using LSTM: {precision_at_K}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
